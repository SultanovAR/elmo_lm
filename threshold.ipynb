{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sultanov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sultanov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/sultanov/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/sultanov/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.bidirectional_lms import elmo_bilm\n",
    "from deeppavlov.models.tokenizers.lazy_tokenizer import LazyTokenizer\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats.mstats import gmean\n",
    "import re\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoAug:\n",
    "    \n",
    "    def __init__(self, model_dir=\"/cephfs/home/sultanov/elmo_lm/lib/python3.6/site-packages/download/bidirectional_lms/elmo_en_news\"):\n",
    "        self.tokenizer = LazyTokenizer()\n",
    "        self.elmo_lm   = elmo_bilm.ELMoEmbedder(model_dir=model_dir)\n",
    "        self.elmo_vocab = np.array(self.elmo_lm.get_vocab())\n",
    "        self.detokenizer = MosesDetokenizer()\n",
    "        self.replacement_patterns = [\n",
    "             (r'won\\'t', 'will not'),\n",
    "             (r'can\\'t', 'cannot'),\n",
    "             (r'i\\'m', 'i am'),\n",
    "             (r'ain\\'t', 'is not'),\n",
    "             (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "             (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "             (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "             (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "             (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "             (r'(\\w+)\\'d', '\\g<1> would')\n",
    "        ]\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in self.replacement_patterns]\n",
    "        self.logger = []\n",
    "        self.num_sent = 0\n",
    "        \n",
    "    def _preproccesing(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "    \n",
    "    def _weighted_sum_distr_by_posistion_in_sent(self, distr):\n",
    "        if len(distr) > 4:\n",
    "            weights = np.array([0.2, 0.35] + [0.5]*(len(distr) - 4) + [0.65, 0.8])\n",
    "        else:\n",
    "            weights = 0.5*np.ones(len(distr))\n",
    "        left = distr[:,0,:]\n",
    "        right = distr[:,1,:]\n",
    "        right = right.transpose([1, 0]) * (1-weights)\n",
    "        right = right.transpose([1, 0])\n",
    "        left = left.transpose([1, 0]) * weights\n",
    "        left = left.transpose([1, 0])\n",
    "        return right + left\n",
    "    \n",
    "    def _get_index_in_vocab(self, token):\n",
    "        indx = np.where(self.elmo_vocab == token)[0]\n",
    "        return indx[0] if indx.size > 0 else None\n",
    "\n",
    "    def _blend_dist(self, batch_distr, num_method):\n",
    "        \"\"\"\n",
    "        blending distr from left and right context\n",
    "        method 0:\n",
    "            sum two distr along left right context\n",
    "        method 1:\n",
    "            weighted sum by place of word in sentence\n",
    "        \"\"\"\n",
    "        if num_method == 0:\n",
    "            return [np.sum(distr, axis=1) for distr in batch_distr]\n",
    "        \n",
    "        elif num_method == 1:\n",
    "            return [self._weighted_sum_distr_by_posistion_in_sent(distr) for distr in batch_distr]\n",
    "        \n",
    "        elif num_method == 2:\n",
    "            return [np.min(distr, axis=1) for distr in batch_distr]\n",
    "        \n",
    "        elif num_method == 3:\n",
    "            return [gmean(distr, axis=1) for distr in batch_distr]\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    def _sent_aug(self, source_sentence: List[str], distr: np.ndarray, threshold: float=0.3, replace_freq: float=0.5):\n",
    "        \"\"\"Sampling words from the distribution\"\"\"\n",
    "        result = []\n",
    "        for i, token in enumerate(source_sentence):\n",
    "            if distr.mask[i].all():\n",
    "                #source token\n",
    "                result.append(token)\n",
    "                log = {'num_sent': self.num_sent, 'num_token': i, 'source': token,\\\n",
    "                       'result': token, 'replaced': False, 'distr': None, 'num_of_candidate': 0}\n",
    "            else:\n",
    "                #replace\n",
    "                idx_word = np.random.choice(len(distr[i]), replace=False, p=self._softmax(distr[i]).filled(0))\n",
    "                result.append(self.elmo_vocab[idx_word])\n",
    "                log = {'num_sent': self.num_sent, 'num_token': i, 'source': token,\\\n",
    "                       'result': self.elmo_vocab[idx_word], 'replaced': True, 'distr': self._softmax(distr[i]).filled(0),\\\n",
    "                       'num_of_candidate': len(distr[i].nonzero()[0]),\\\n",
    "                       'candidates': self.elmo_vocab[distr[i].nonzero()[0]]}\n",
    "            self.logger.append(log)\n",
    "        self.num_sent += 1\n",
    "        return result\n",
    "    \n",
    "    def _get_threshold_masked(self, data, indx_source_token, threshold, replace_freq):\n",
    "        \"\"\"\n",
    "        Creating mask:\n",
    "            source word - deleted\n",
    "            word with probability less that threshold - deleted\n",
    "            amount of words that will be replaced â‰ˆ replace_freq * len(sentence)\n",
    "            if amount of words with probability > threshold <= replace_freq * len(sentence)\n",
    "            then will printed warning and all words with probability > threshold will be replaced\n",
    "        \"\"\"\n",
    "        #Creating a mask based on threshold\n",
    "        assert(len(data) == len(indx_source_token))\n",
    "        mask = data > threshold\n",
    "        #Creating a mask that marks the source tokens, and merging it with previous mask\n",
    "        onehot_indx = np.zeros((len(indx_source_token), len(el.elmo_vocab)))\n",
    "        for i, indx in enumerate(indx_source_token):\n",
    "            if indx:\n",
    "                onehot_indx[i, indx] = 1\n",
    "        mask = (mask * ~(onehot_indx).astype(bool)).astype(bool)\n",
    "        #Creating a mask based on replace frequence, and merging it with previous\n",
    "        word_mask = mask.any(axis=1)\n",
    "        if replace_freq >= len(word_mask.nonzero()[0])/len(word_mask):\n",
    "            print('warning')\n",
    "            word_mask = word_mask\n",
    "        else:\n",
    "            freq = (replace_freq * len(word_mask)) / len(word_mask.nonzero()[0])\n",
    "            word_mask = (np.random.binomial(1, p=freq, size=(len(word_mask))) * word_mask).astype(bool)\n",
    "        mask = (mask.T*word_mask).T\n",
    "        return np.ma.masked_array(data=data, mask=~mask, fill_value=np.nan)\n",
    "        \n",
    "    def _batch_sent(self, batch_sent: List[str], threshold: float, replace_freq: float, num_method_blend: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Replaces some words in the original sentence with words from the language model with frequency p\n",
    "        Args:\n",
    "            batch_token: Sentences to be augmented \n",
    "            threshold: words with probability < threshold will not be considerated like replacement\n",
    "            p: frequency of replacing words\n",
    "            num_method_blend: method of merging two distributions: left-context and right-context\n",
    "        Returns: \n",
    "            Contains the augmented sentences\n",
    "        \"\"\"\n",
    "        batch_sent_prep         = [self._preproccesing(sent) for sent in batch_sent]\n",
    "        batch_token             = self.tokenizer(batch_sent_prep)\n",
    "        batch_indx_source_token = [np.array(list(map(self._get_index_in_vocab, sent))) for sent in batch_token]\n",
    "        batch_distr             = self._blend_dist(self.elmo_lm(batch_token), num_method_blend)\n",
    "        batch_mask_distr        = [self._get_threshold_masked(batch_distr[i], batch_indx_source_token[i], threshold, replace_freq) for i in range(len(batch_token))]\n",
    "        self.batch_mask_distr = batch_mask_distr\n",
    "        batch_aug_token         = [self._sent_aug(batch_token[i], batch_mask_distr[i]) for i in range(len(batch_token))]\n",
    "        return batch_aug_token        \n",
    "    \n",
    "    def __call__(self, batch_sent: List[str], threshold: float, replace_freq: float, num_method_blend: int):\n",
    "        batch_aug_token = self._batch_sent(batch_sent, threshold, replace_freq, num_method_blend)\n",
    "        return [self.detokenizer.detokenize(i, return_str=True) for i in batch_aug_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "/cephfs/home/sultanov/elmo_lm/lib/python3.6/site-packages/download/bidirectional_lms/elmo_en_news\n",
      "WARNING:tensorflow:From /cephfs/home/sultanov/elmo_lm/lib/python3.6/site-packages/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-31 19:19:24.930 WARNING in 'tensorflow'['tf_logging'] at line 125: From /cephfs/home/sultanov/elmo_lm/lib/python3.6/site-packages/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "INFO:tensorflow:Restoring parameters from /cephfs/home/sultanov/elmo_lm/lib/python3.6/site-packages/download/bidirectional_lms/elmo_en_news/model.ckpt-935588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-31 19:19:25.564 INFO in 'tensorflow'['tf_logging'] at line 115: Restoring parameters from /cephfs/home/sultanov/elmo_lm/lib/python3.6/site-packages/download/bidirectional_lms/elmo_en_news/model.ckpt-935588\n"
     ]
    }
   ],
   "source": [
    "el = ElmoAug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = \\\n",
    "[\"Almost half of all iPhone owners have broken their screens, not just once but an average of two times each.\",\\\n",
    "   \"i really don't understand your point.\\xa0 It seems that you are mixing apples and oranges.\",\\\n",
    "   \"shut the fuck up. you and the rest of your faggot friends should be burned at the stake\",\\\n",
    "   \"That you are an idiot who understands neither taxation nor women's health.\",\\\n",
    "   \"What on Earth is that about? Is it what's going to get him fired eventually?\",\\\n",
    "   \"This is a doctrine of constitutional interpretation that says that a constitution is organic and must be read in a broad and liberal manner so as to adapt it to changing times.\",\\\n",
    "   \"In the 2000s, music notation typically means the written expression of music notes and rhythms on paper using symbols.\",\\\n",
    "   \"Most of the mathematical notation in use today was not invented until the 16th century.[52] Before that, mathematics was written out in words, limiting mathematical discovery.\",\\\n",
    "   \"Physical geography deals with the study of processes and patterns in the natural environment like the atmosphere, hydrosphere, biosphere, and geosphere.\",\\\n",
    "   \"An autobiography is written by the person himself or herself, sometimes with the assistance of a collaborator or ghostwriter.\",\\\n",
    "    \"You fuck your dad.\",\\\n",
    "    \"Yeah and where are you now?\",\\\n",
    "    \"shut the fuck up. you and the rest of your faggot friends should be burned at the stake\",\\\n",
    "    \"you are a land creature. You would drown....\",\\\n",
    "    \"But how would you actually get the key out?\",\\\n",
    "    \"fucking behave then you prick!\",\\\n",
    "    \"You right if you are relaxe then you can give better result or perform and your identity should be from your work.\",\\\n",
    "    \"The laughs you two heard were triggered by memories of his own high-flying exits off moving beasts\",\\\n",
    " \"Well, you guys have gone and done it now. You put the words 'China' and 'Chinese' up the required number of times for the dating Asians ad to come up. Evidently, Ms. Zhang, 50Kg and 168cm [for a BMI of 17.8] from 'HuNan China' wants to meet me. She has her little mouth open like she's speaking. What's that you ask, Zhang? Well, yes, as a matter of fact I am a physician.  Why are you clapping your hands together and jumping up and down?  Stop that squealing, young lady and 'exprain' yourself!\",\\\n",
    " \"Fact : Georgia passed a strict immigration policy and most of the Latino farm workers left the area. Vidalia Georgia now has over 3000 agriculture job openings and they have been able to fill about 250 of them in past year. All you White Real Americans who are looking for work that the Latinos stole from you..Where are you ? The jobs are i Vadalia just waiting for you..Or maybe its the fact that you would rather collect unemployment like the rest of the Tea Klaners.. You scream..you complain..and you sit at home in your wife beaters and drink beer..Typical Real White Tea Klan...\"\n",
    "]\n",
    "len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 54min 55s, sys: 39min 30s, total: 3h 34min 25s\n",
      "Wall time: 4min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['But three-quarters of all iPhone owners have hit his date two not merely on but an most of few times each.',\n",
       " 'They really do still understand your hand., seems for you are mixing apples and <UNK> from',\n",
       " 'Obama the two up., and the good of your faggot friends should be burned at the stake',\n",
       " 'That you are an official really knows about taxation for women ( health \"',\n",
       " \"How on hell are that about 'Is it a really just to get him's this?\",\n",
       " 'This of this study of The interpretation that says that a policy even and and must be done in a broad and straightforward one \"as or adapt -- in changing and (',\n",
       " 'In the 2006, music notation typically only the written expression of music notes and rhythms on paper using symbols from',\n",
       " ', now the mathematical notation at use today was not built until this 16th century -- \"12] Before that, writing was written and in words, limiting mathematical,.',\n",
       " 'Physical geography - with the study like processes but material including the natural settings like the atmosphere and power and food, and,.',\n",
       " 'that order ( written by their person himself or herself, \"with the benefit or a collaborator or the.',\n",
       " \"Parents may old dad '\",\n",
       " 'I and please are you just?',\n",
       " 'shut the stock up. you and the, are your old friends should be burned that the stake',\n",
       " 'you are a land open; You would disagree:.',\n",
       " 'But how would she <UNK> want the key benefit?',\n",
       " 'fucking does \"you prick!',\n",
       " 'You right like you are relaxe then you are you better ( or perform and your identity should work about your work before',\n",
       " 'Those laughs you two heard those triggered and memories of his new in, off moving beasts',\n",
       " 'Well, this, have -- and done enough?. \"put the <UNK>\\'China on and\\'Chinese\\'s along <UNK> record good as times for their... search ad time come again. Evidently out Ms. and, a and above [ to 2 BMI of 17.8] from\\'HuNan China. goes to meet me. She has her and nose open like she or speaking. What is why do ask, Zhang? And, yes the as a matter, <UNK> and saw <UNK> physician <UNK> Why were\" <UNK> your \"together and jumping up and in\" Stop something about, it lady and\\'exprain bring is!',\n",
       " \"Fact: Georgia had a strict new and for most of the Latino farm drivers left the area. Vidalia also now's over 3000 agriculture industry openings and few have not hoping to fill by 40,000 of them of past week. Just The White will Americans who like voted at is that the obama stole from you..Where are you? The right are a feel) waiting through you..Or maybe of the fact that <UNK> really rather wage unemployment like... the or the real) just <UNK> complain..and will sit back home calling 'wife, and... beer..Typical of White Tea Klan...\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "el(test_sentences, 3e-4, 0.4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (pd.DataFrame(el.logger))\n",
    "d.to_csv('./result_last.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = ['But three-quarters of all iPhone owners have hit his date two not merely on but an most of few times each.',\n",
    " 'They really do still understand your hand., seems for you are mixing apples and <UNK> from',\n",
    " 'Obama the two up., and the good of your faggot friends should be burned at the stake',\n",
    " 'That you are an official really knows about taxation for women ( health \"',\n",
    " \"How on hell are that about 'Is it a really just to get him's this?\",\n",
    " 'This of this study of The interpretation that says that a policy even and and must be done in a broad and straightforward one \"as or adapt -- in changing and (',\n",
    " 'In the 2006, music notation typically only the written expression of music notes and rhythms on paper using symbols from',\n",
    " ', now the mathematical notation at use today was not built until this 16th century -- \"12] Before that, writing was written and in words, limiting mathematical,.',\n",
    " 'Physical geography - with the study like processes but material including the natural settings like the atmosphere and power and food, and,.',\n",
    " 'that order ( written by their person himself or herself, \"with the benefit or a collaborator or the.',\n",
    " \"Parents may old dad '\",\n",
    " 'I and please are you just?',\n",
    " 'shut the stock up. you and the, are your old friends should be burned that the stake',\n",
    " 'you are a land open; You would disagree:.',\n",
    " 'But how would she <UNK> want the key benefit?',\n",
    " 'fucking does \"you prick!',\n",
    " 'You right like you are relaxe then you are you better ( or perform and your identity should work about your work before',\n",
    " 'Those laughs you two heard those triggered and memories of his new in, off moving beasts',\n",
    " 'Well, this, have -- and done enough?. \"put the <UNK>\\'China on and\\'Chinese\\'s along <UNK> record good as times for their... search ad time come again. Evidently out Ms. and, a and above [ to 2 BMI of 17.8] from\\'HuNan China. goes to meet me. She has her and nose open like she or speaking. What is why do ask, Zhang? And, yes the as a matter, <UNK> and saw <UNK> physician <UNK> Why were\" <UNK> your \"together and jumping up and in\" Stop something about, it lady and\\'exprain bring is!',\n",
    " \"Fact: Georgia had a strict new and for most of the Latino farm drivers left the area. Vidalia also now's over 3000 agriculture industry openings and few have not hoping to fill by 40,000 of them of past week. Just The White will Americans who like voted at is that the obama stole from you..Where are you? The right are a feel) waiting through you..Or maybe of the fact that <UNK> really rather wage unemployment like... the or the real) just <UNK> complain..and will sit back home calling 'wife, and... beer..Typical of White Tea Klan...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ex_last.txt', 'w') as f:\n",
    "    for line in\n",
    "    f.write(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
